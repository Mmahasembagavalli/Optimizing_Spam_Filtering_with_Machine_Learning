# -*- coding: utf-8 -*-
"""Optimizing_Spam_Filtering_with_Machine_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1heQqBMLT7oeqHu28qZWetYBBe5nf_96q
"""

import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()

"""You can import your own data into Colab notebooks from your Google Drive account, including from spreadsheets, as well as from Github and many other sources. To learn more about importing data, and how Colab can be used for data science, see the links below under [Working with Data](#working-with-data).

<div class="markdown-google-sans">

## Machine learning
</div>

With Colab you can import an image dataset, train an image classifier on it, and evaluate the model, all in just [a few lines of code](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb). Colab notebooks execute code on Google's cloud servers, meaning you can leverage the power of Google hardware, including [GPUs and TPUs](#using-accelerated-hardware), regardless of the power of your machine. All you need is a browser.
"""

import pandas as pd

df = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv', encoding="latin-1")

df.head()

df.tail()

df = df.dropna(how="any", axis=1)

df.columns = ['target', 'message']

df.head()

df.tail()

df.shape

df.columns

df.duplicated().sum()

df = df.drop_duplicates()

df.isnull().sum()

df.nunique()

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
df['num_char'] = df['message'].apply(len)
df.head()

df["num_wd"] = df.apply(lambda row: nltk.word_tokenize(row["message"]), axis=1).apply(len)
df["num_sn"] = df.apply(lambda row: nltk.sent_tokenize(row["message"]), axis=1).apply(len)
df.head()

df.head()

df.describe().T.style.background_gradient(cmap = "Purples_r")

import matplotlib.pyplot as plt
import seaborn as sns
from plotly import graph_objs as go
import plotly.express as px
import plotly.figure_factory as ff
import warnings
warnings.filterwarnings('ignore')

df['target'].unique()

df['target'].value_counts()

plt.figure(figsize=(18, 8))
plt.subplot(1, 2, 1)
df.target.value_counts().plot(kind="pie",
                          fontsize=16,                  
                          labels=["Spam", "Ham"],
                          ylabel="Spam vs Ham",
                          autopct='%1.1f%%');

plt.subplot(1, 2, 2)
sns.countplot(x="target",data=df, palette="pastel")
plt.show()

balance_counts = df.groupby('target')['target'].agg('count').values
balance_counts

fig = go.Figure()
fig.add_trace(go.Bar(
    x=['ham'],
    y=[balance_counts[0]],
    name='ham',
    text=[balance_counts[0]],
    textposition='auto',
    marker_color= 'blue'
))
fig.add_trace(go.Bar(
    x=['spam'],
    y=[balance_counts[1]],
    name='spam',
    text=[balance_counts[1]],
    textposition='auto',
    marker_color= 'red'
))
fig.update_layout(
    title='<span style="font-size:32px; font-family:Times New Roman">Dataset distribution by target</span>'
)
fig.show()

ham_df = df[df['target'] == 'ham']['num_wd'].value_counts().sort_index()
spam_df = df[df['target'] == 'spam']['num_wd'].value_counts().sort_index()

fig = go.Figure()
fig.add_trace(go.Scatter(
    x=ham_df.index,
    y=ham_df.values,
    name='ham',
    fill='tozeroy',
    marker_color= 'blue',
))
fig.add_trace(go.Scatter(
    x=spam_df.index,
    y=spam_df.values,
    name='spam',
    fill='tozeroy',
    marker_color= 'red',
))
fig.update_layout(
    title='<span style="font-size:32px; font-family:Times New Roman">Data Distribution in Different Fields</span>'
)
fig.update_xaxes(range=[0, 70])
fig.show()

#https://www.kaggle.com/code/casper6290/sms-spam-ham-99-acc?cellIds=29&kernelSessionId=101302412
import numpy as np
view=df.groupby('target').agg([np.mean,np.std,len,np.min,np.max])
view.plot(kind='bar',figsize=(15,8),title='Comparision between the classes',logy=True)

sns.pairplot(df,hue='target')

def clean_text(text):
    text = text.lower() 
    return text.strip()

df['new_message'] = df.message.apply(lambda x: clean_text(x))

import string
string.punctuation

def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree
df['new_message']= df['new_message'].apply(lambda x:remove_punctuation(x))

import re
def tokenization(text):
    tokens = re.split('W+',text)
    return tokens
df['new_message']= df['new_message'].apply(lambda x: tokenization(x))

import nltk
stopwords = nltk.corpus.stopwords.words('english')
stopwords[0:10]
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're"]

def remove_stopwords(text):
    output= " ".join(i for i in text if i not in stopwords)
    return output

df['new_message']= df['new_message'].apply(lambda x:remove_stopwords(x))

from nltk.stem.porter import PorterStemmer
porter_stemmer = PorterStemmer()

def stemming(text):
    stem_text = "".join([porter_stemmer.stem(word) for word in text])
    return stem_text
df['new_message']=df['new_message'].apply(lambda x: stemming(x))

from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
from spacy import load

nltk.download('wordnet')
nltk.download('wordnet2022')
nlp = load('en_core_web_sm')
[nltk_data] Downloading package wordnet to /usr/share/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package wordnet2022 to /usr/share/nltk_data...
[nltk_data]   Unzipping corpora/wordnet2022.zip.

def lemmatizer(text):
    lemm_text = "".join([wordnet_lemmatizer.lemmatize(word) for word in text])
    return lemm_text
df['new_message']=df['new_message'].apply(lambda x:lemmatizer(x))

def clean_text(text):
    text = re.sub('\[.*\]','', text).strip() # Remove text in square brackets
    text = re.sub('\S*\d\S*\s*','', text).strip()  # Remove words containing numbers
    return text.strip()

df['new_message'] = df.new_message.apply(lambda x: clean_text(x))

stopwords = nlp.Defaults.stop_words
def lemmatizer(text):
    doc = nlp(text)
    sent = [token.lemma_ for token in doc if not token.text in set(stopwords)]
    return ' '.join(sent)

df['new_message'] =  df.new_message.apply(lambda x: lemmatizer(x))

def remove_urls(vTEXT):
    vTEXT = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '', vTEXT, flags=re.MULTILINE)
    return(vTEXT)

df['new_message'] = df.new_message.apply(lambda x: remove_urls(x))

def remove_digits(text):
    clean_text = re.sub(r"\b[0-9]+\b\s*", "", text)
    return(text)

df['new_message'] = df.new_message.apply(lambda x: remove_digits(x))

def remove_digits1(sample_text):
    clean_text = " ".join([w for w in sample_text.split() if not w.isdigit()]) # Side effect: removes extra spaces
    return(clean_text)

df['new_message'] = df.new_message.apply(lambda x: remove_digits1(x))

def remove_emojis(data):
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               "]+", flags=re.UNICODE)
    return re.sub(emoji_pattern, '', data)

df['new_message'] = df.new_message.apply(lambda x: remove_emojis(x))

df['new_message']

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le.fit(df['target'])

df['target_encoded'] = le.transform(df['target'])
df.head()

from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

twitter_mask = np.array(Image.open('/kaggle/input/maskwordclud22/Mask-wordcloud/twitter_mask3.jpg'))

wc = WordCloud(
    background_color='white', 
    max_words=200, 
    mask=twitter_mask,
)
wc.generate(' '.join(text for text in df.loc[df['target'] == 'spam', 'message']))
plt.figure(figsize=(18,10))
plt.title('Top words for Spam messages', 
          fontdict={'size': 22,  'verticalalignment': 'bottom'})
plt.imshow(wc)
plt.axis("off")
plt.show()

twitter_mask = np.array(Image.open('/kaggle/input/maskwordclud22/Mask-wordcloud/twitter_mask3.jpg'))

wc = WordCloud(
    background_color='white', 
    max_words=200, 
    mask=twitter_mask,
)
wc.generate(' '.join(text for text in df.loc[df['target'] == 'ham', 'message']))
plt.figure(figsize=(18,10))
plt.title('Top words for Ham messages', 
          fontdict={'size': 22,  'verticalalignment': 'bottom'})
plt.imshow(wc)
plt.axis("off")
plt.show()

#https://www.kaggle.com/code/karanchinchpure/nlp-pipeline-details-explained#1.1-Feature-Engineering
#create spam corpus which will holds all Spam words
spam_corpus = []
for msg in df[df['target_encoded'] == 1]['message'].tolist():
    for word in msg.split():
        spam_corpus.append(word)
from collections import Counter
plt.figure(figsize=(10,7))
sns.barplot(y=pd.DataFrame(Counter(spam_corpus).most_common(25))[0],x=pd.DataFrame(Counter(spam_corpus).most_common(30))[1])
plt.xticks()
plt.title("Most Commonly Used Ham Words")
plt.xlabel("Frequnecy")
plt.ylabel("Ham Words")
plt.show()

#https://www.kaggle.com/code/karanchinchpure/nlp-pipeline-details-explained#1.1-Feature-Engineering
#create spam corpus which will holds all Ham or Non Spam words
ham_corpus = []
for msg in df[df['target_encoded'] == 0]['message'].tolist():
    for word in msg.split():
        ham_corpus.append(word)
from collections import Counter
plt.figure(figsize=(10,7))
sns.barplot(y=pd.DataFrame(Counter(ham_corpus).most_common(25))[0],x=pd.DataFrame(Counter(ham_corpus).most_common(30))[1])
plt.xticks()
plt.title("Most Commonly Used Spam Words")
plt.xlabel("Frequnecy")
plt.ylabel("Spam Words")
plt.show()

#https://www.kaggle.com/code/karanchinchpure/nlp-pipeline-details-explained#1.1-Feature-Engineering
#Characters Visualize
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,6))
text_len=df[df['target_encoded']==1]['num_char']
ax1.hist(text_len,color='green')
ax1.set_title('Original text')
text_len=df[df['target_encoded']==0]['num_char']
ax2.hist(text_len,color='red')
ax2.set_title('Fake text')
fig.suptitle('Characters in texts')
plt.show()

#https://www.kaggle.com/code/karanchinchpure/nlp-pipeline-details-explained#1.1-Feature-Engineering
#Words Visualize
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,6))
text_len=df[df['target_encoded']==1]['num_wd']
ax1.hist(text_len,color='green')
ax1.set_title('Original text')
text_len=df[df['target_encoded']==0]['num_wd']
ax2.hist(text_len,color='red')
ax2.set_title('Fake text')
fig.suptitle('Words in texts')
plt.show()

x = df['new_message']
y = df['target_encoded']

print(len(x), len(y))
5169 5169

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)
print(len(x_train), len(y_train))
print(len(x_test), len(y_test))
3876 3876
1293 1293

from sklearn.feature_extraction.text import CountVectorizer

vect = CountVectorizer()
vect.fit(x_train)

x_train_dtm = vect.transform(x_train)
x_test_dtm = vect.transform(x_test)

vect_tunned = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.1, max_df=0.7, max_features=100)

from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer()

tfidf_transformer.fit(x_train_dtm)
x_train_tfidf = tfidf_transformer.transform(x_train_dtm)

x_train_tfidf

texts = df['new_message']
target = df['target_encoded']

from keras.preprocessing.text import Tokenizer

word_tokenizer = Tokenizer()
word_tokenizer.fit_on_texts(texts)

vocab_length = len(word_tokenizer.word_index) + 1
vocab_length

import tensorflow as tf

from tensorflow.keras.preprocessing.sequence import pad_sequences

from nltk.tokenize import word_tokenize

def embed(corpus): 
    return word_tokenizer.texts_to_sequences(corpus)

longest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))
length_long_sentence = len(word_tokenize(longest_train))

train_padded_sentences = pad_sequences(
    embed(texts), 
    length_long_sentence, 
    padding='post'
)

train_padded_sentence

embeddings_dictionary = dict()
embedding_dim = 100

# Load GloVe 100D embeddings
with open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt', encoding="utf8") as fp:
    for line in fp.readlines():
        records = line.split()
        word = records[0]
        vector_dimensions = np.asarray(records[1:], dtype='float32')
        embeddings_dictionary [word] = vector_dimensions
# embeddings_dictionary

# Now we will load embedding vectors of those words that appear in the
# Glove dictionary. Others will be initialized to 0.
embedding_matrix = np.zeros((vocab_length, embedding_dim))

for word, index in word_tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector
        
embedding_matrix

import plotly.figure_factory as ff

x_axes = ['Ham', 'Spam']
y_axes =  ['Spam', 'Ham']

def conf_matrix(z, x=x_axes, y=y_axes):
    
    z = np.flip(z, 0)

    # change each element of z to type string for annotations
    z_text = [[str(y) for y in x] for x in z]

    # set up figure 
    fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')

    # add title
    fig.update_layout(title_text='<b>Confusion matrix</b>',
                      xaxis = dict(title='Predicted value'),
                      yaxis = dict(title='Real value')
                     )

    # add colorbar
    fig['data'][0]['showscale'] = True
    
    return fig

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

# Train the model
nb.fit(x_train_dtm, y_train)

y_pred_class = nb.predict(x_test_dtm)
y_pred_prob = nb.predict_proba(x_test_dtm)[:, 1]

from sklearn import metrics
print(metrics.accuracy_score(y_test, y_pred_class))

conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))
0.9752513534416086

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline

pipe = Pipeline([('bow', CountVectorizer()), 
                 ('tfid', TfidfTransformer()),  
                 ('model', MultinomialNB())])

pipe.fit(x_train, y_train)

y_pred_class = pipe.predict(x_test)

print(metrics.accuracy_score(y_test, y_pred_class))

conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))
0.9551430781129157

import xgboost as xgb

pipe = Pipeline([
    ('bow', CountVectorizer()), 
    ('tfid', TfidfTransformer()),  
    ('model', xgb.XGBClassifier(
        learning_rate=0.1,
        max_depth=7,
        n_estimators=80,
        use_label_encoder=False,
        eval_metric='auc',
        colsample_bytree=0.8,
        subsample=0.7,
        min_child_weight=5,
    ))
])

pipe.fit(x_train, y_train)

y_pred_class = pipe.predict(x_test)
y_pred_train = pipe.predict(x_train)

print('Train: {}'.format(metrics.accuracy_score(y_train, y_pred_train)))
print('Test: {}'.format(metrics.accuracy_score(y_test, y_pred_class)))

conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))
Train: 0.9574303405572755
Test: 0.94276875483372

from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline

pipe = Pipeline([('bow', CountVectorizer()), 
                 ('tfid', TfidfTransformer()),  
                 ('model', RandomForestClassifier(n_estimators=50, random_state=2))])

pipe.fit(x_train, y_train)

y_pred_class = pipe.predict(x_test)

print(metrics.accuracy_score(y_test, y_pred_class))

conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))
0.9651972157772621

from sklearn.svm import SVC
from sklearn.pipeline import Pipeline

pipe = Pipeline([('bow', CountVectorizer()), 
                 ('tfid', TfidfTransformer()),  
                 ('model', SVC(kernel='sigmoid', gamma=1.0))])

pipe.fit(x_train, y_train)

y_pred_class = pipe.predict(x_test)

print(metrics.accuracy_score(y_test, y_pred_class))

conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))
0.9791183294663574

X_train, X_test, y_train, y_test = train_test_split(
    train_padded_sentences, 
    target, 
    test_size=0.25
)

import tensorflow
import keras
from keras.models import Sequential
from keras.initializers import Constant
from keras.layers import (LSTM, 
                          Embedding, 
                          BatchNormalization,
                          Dense, 
                          TimeDistributed, 
                          Dropout, 
                          Bidirectional,
                          Flatten, 
                          GlobalMaxPool1D)
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Embedding
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

from sklearn.metrics import (
    precision_score, 
    recall_score, 
    f1_score, 
    classification_report,
    accuracy_score
)import tensorflow
import keras
from keras.models import Sequential
from keras.initializers import Constant
from keras.layers import (LSTM, 
                          Embedding, 
                          BatchNormalization,
                          Dense, 
                          TimeDistributed, 
                          Dropout, 
                          Bidirectional,
                          Flatten, 
                          GlobalMaxPool1D)
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Embedding
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

from sklearn.metrics import (
    precision_score, 
    recall_score, 
    f1_score, 
    classification_report,
    accuracy_score
)

def glove_lstm():
    model = Sequential()
    
    model.add(Embedding(
        input_dim=embedding_matrix.shape[0], 
        output_dim=embedding_matrix.shape[1], 
        weights = [embedding_matrix], 
        input_length=length_long_sentence
    ))
    
    model.add(Bidirectional(LSTM(
        length_long_sentence, 
        return_sequences = True, 
        recurrent_dropout=0.2
    )))
    
    model.add(GlobalMaxPool1D())
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(length_long_sentence, activation = "relu"))
    model.add(Dropout(0.5))
    model.add(Dense(length_long_sentence, activation = "relu"))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation = 'sigmoid'))
    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
    
    return model

model = glove_lstm()
model.summary()
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (None, 69, 100)           696400    
                                                                 
 bidirectional (Bidirectiona  (None, 69, 138)          93840     
 l)                                                              
                                                                 
 global_max_pooling1d (Globa  (None, 138)              0         
 lMaxPooling1D)                                                  
                                                                 
 batch_normalization (BatchN  (None, 138)              552       
 ormalization)                                                   
                                                                 
 dropout (Dropout)           (None, 138)               0         
                                                                 
 dense (Dense)               (None, 69)                9591      
                                                                 
 dropout_1 (Dropout)         (None, 69)                0         
                                                                 
 dense_1 (Dense)             (None, 69)                4830      
                                                                 
 dropout_2 (Dropout)         (None, 69)                0         
                                                                 
 dense_2 (Dense)             (None, 1)                 70        
                                                                 
=================================================================
Total params: 805,283
Trainable params: 805,007
Non-trainable params: 276
_________________________________________________________________

model = glove_lstm()

checkpoint = ModelCheckpoint(
    'model.h5', 
    monitor = 'val_loss', 
    verbose = 1, 
    save_best_only = True
)
reduce_lr = ReduceLROnPlateau(
    monitor = 'val_loss', 
    factor = 0.2, 
    verbose = 1, 
    patience = 5,                        
    min_lr = 0.001
)
history = model.fit(
    X_train, 
    y_train, 
    epochs = 3,
    batch_size = 32,
    validation_data = (X_test, y_test),
    verbose = 1,
    callbacks = [reduce_lr, checkpoint]
)
Epoch 1/3
122/122 [==============================] - ETA: 0s - loss: 0.3761 - accuracy: 0.8638
Epoch 1: val_loss improved from inf to 0.32021, saving model to model.h5
122/122 [==============================] - 23s 134ms/step - loss: 0.3761 - accuracy: 0.8638 - val_loss: 0.3202 - val_accuracy: 0.9242 - lr: 0.0010
Epoch 2/3
122/122 [==============================] - ETA: 0s - loss: 0.2140 - accuracy: 0.9223
Epoch 2: val_loss improved from 0.32021 to 0.24257, saving model to model.h5
122/122 [==============================] - 16s 128ms/step - loss: 0.2140 - accuracy: 0.9223 - val_loss: 0.2426 - val_accuracy: 0.9288 - lr: 0.0010
Epoch 3/3
122/122 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9469
Epoch 3: val_loss improved from 0.24257 to 0.20176, saving model to model.h5
122/122 [==============================] - 15s 126ms/step - loss: 0.1673 - accuracy: 0.9469 - val_loss: 0.2018 - val_accuracy: 0.9312 - lr: 0.0010

def plot_learning_curves(history, arr):
    fig, ax = plt.subplots(1, 2, figsize=(20, 5))
    for idx in range(2):
        ax[idx].plot(history.history[arr[idx][0]])
        ax[idx].plot(history.history[arr[idx][1]])
        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)
        ax[idx].set_xlabel('A ',fontsize=16)
        ax[idx].set_ylabel('B',fontsize=16)
        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)

y_preds = (model.predict(X_test) > 0.5).astype("int32")
conf_matrix(metrics.confusion_matrix(y_test, y_preds))

import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint

import transformers
from tqdm.notebook import tqdm
from tokenizers import BertWordPieceTokenizer

from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')
Downloading (…)solve/main/vocab.txt: 100%
232k/232k [00:00<00:00, 4.96MB/s]
Downloading (…)okenizer_config.json: 100%
28.0/28.0 [00:00<00:00, 1.58kB/s]
Downloading (…)lve/main/config.json: 100%
571/571 [00:00<00:00, 27.3kB/s]

def bert_encode(data, maximum_length) :
    input_ids = []
    attention_masks = []

    for text in data:
        encoded = tokenizer.encode_plus(
            text, 
            add_special_tokens=True,
            max_length=maximum_length,
            pad_to_max_length=True,

            return_attention_mask=True,
        )
        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])
        
    return np.array(input_ids),np.array(attention_masks)

texts = df['new_message']
target = df['target_encoded']

train_input_ids, train_attention_masks = bert_encode(texts,60)
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.

import tensorflow as tf
from tensorflow.keras.optimizers import Adam

def create_model(bert_model):
    
    input_ids = tf.keras.Input(shape=(60,),dtype='int32')
    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')

    output = bert_model([input_ids,attention_masks])
    output = output[1]
    output = tf.keras.layers.Dense(32,activation='relu')(output)
    output = tf.keras.layers.Dropout(0.2)(output)
    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)
    
    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)
    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])
    return model

from transformers import TFBertModel
bert_model = TFBertModel.from_pretrained('bert-base-uncased')
Downloading (…)lve/main/config.json: 100%
570/570 [00:00<00:00, 32.7kB/s]
Downloading tf_model.h5: 100%
536M/536M [00:08<00:00, 66.3MB/s]
Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.

model = create_model(bert_model)
model.summary()
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 60)]         0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 60)]         0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                
                                thPoolingAndCrossAt               'input_2[0][0]']                
                                tentions(last_hidde                                               
                                n_state=(None, 60,                                                
                                768),                                                             
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dense_6 (Dense)                (None, 32)           24608       ['tf_bert_model[0][1]']          
                                                                                                  
 dropout_43 (Dropout)           (None, 32)           0           ['dense_6[0][0]']                
                                                                                                  
 dense_7 (Dense)                (None, 1)            33          ['dropout_43[0][0]']             
                                                                                                  
==================================================================================================
Total params: 109,506,881
Trainable params: 109,506,881
Non-trainable params: 0
__________________________________________________________________________________________________

history = model.fit(
    [train_input_ids, train_attention_masks],
    target,
    validation_split=0.2, 
    epochs=3,
    batch_size=10
)
Epoch 1/3
414/414 [==============================] - 1769s 4s/step - loss: 0.4240 - accuracy: 0.8687 - val_loss: 0.3536 - val_accuracy: 0.8868
Epoch 2/3
414/414 [==============================] - 1725s 4s/step - loss: 0.4033 - accuracy: 0.8704 - val_loss: 0.3576 - val_accuracy: 0.8868
Epoch 3/3
414/414 [==============================] - 1729s 4s/step - loss: 0.4076 - accuracy: 0.8704 - val_loss: 0.3559 - val_accuracy: 0.8868

plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])







